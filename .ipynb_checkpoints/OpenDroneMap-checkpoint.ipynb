{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using professional grade SFM software\n",
    "\n",
    "We've spent a substantial amount of time developing code that can produce point clouds from 2 (and 3) images.  In this process, I hope that you have some sense of how this could generalize to developing point clouds from $n$ images.  This isn't terribly difficult, but there is a point at which code needs to be optimized to run such methods in a reasonable amount of time.  Similarly, there are mechanisms that need to be put in place to ensure that feature matching is robust, that outliers are correctly detected and removed, etc.  Furthermore, to obtain a visualization that is satisfying, we need to determine how to map a 2D image to the resulting 3D image in a continuous fashion.  As with most computational tasks, true professional grade solutions require a deep dive that is outside the scope of an introductory course.\n",
    "\n",
    "Fortunately, there exists a substantial body of open-source code that will solve the SfM problem in a satisfying way, while also being quite transparent.  The particular software that we will use is called OpenDroneMap.  OpenDroneMap is sort of a meta-package: it doesn't implement anything itself, but rather integrates a bunch of other existing open-source software into an easy-to-use pipeline.  It's easy to call, and runs inside a [docker container](https://www.docker.com/).  In its default configuration, it will run a variety of algorithms and produce a number of cool outputs, some familiar, some less so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -ti --rm -v $PWD/boot:/datasets/code opendronemap/opendronemap --project-path /datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above command, there are a few tricks in the syntax.  First, note the exclamation point at the beginning of the line: this tells ipython to run the command in bash rather than in python (I'm not sure how this plays with windows, some experimentation may be needed).  Second, the path on the left side of the colon is the path to the location of the ODM project: inside this directory should already be a file called *images* that contains all the images you want to run SfM on.  The script will populate this directory with a bunch of outputs as well.  This docker command is nice because it will download the docker file automatically if it's not found locally.  \n",
    "\n",
    "The command above will run for awhile (or not if you've already run it once).  Let's look at what it produced: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/brinkerhoff/Sandbox/cv_sfm/boot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic product here is contained in the directory *opensfm*.  The first subpackage that ODM calls is a software library called OpenSFM, which performs robust structure from motion yielding a very nice point cloud, in the form of a [json](https://en.wikipedia.org/wiki/JSON) file.  If you open the file undistorted_reconstruction.json, you'll find a readable text format that looks alot like a python dictionary.  Indeed, python has a nice library for reading json files into a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/brinkerhoff/Sandbox/cv_sfm/boot/opensfm/undistorted_reconstruction.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit too much to deal with.  If we want to visualize this point cloud, we just want the 3D point coordinates and the color of the underlying image at that point.  We can pull these out of the json file and plot them like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "point_coordinates = []\n",
    "colors = []\n",
    "for p in data[0]['points']:\n",
    "    point_coordinates.append(data[0]['points'][p]['coordinates'])  \n",
    "    colors.append(data[0]['points'][p]['color'])\n",
    "X = np.array(point_coordinates)\n",
    "C = np.array(colors)/255\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,1],X[:,2],c=C)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't really any different from the SfM code that you've written, just a bit more robust and set up to handle more images and bigger problems.  What else does ODM produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/brinkerhoff/Sandbox/cv_sfm/boot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing a point cloud, ODM passes this off to a meshing routine.  When meshing, the software will build a sensible set of triangles between points in the image, so that we move from an unstructured set of points in 3D, to a complete *2D surface* embedded in 3D.  This is also known as a manifold.  We can easily look at the mesh produced by ODM by using the open-source software [meshlab](http://www.meshlab.net/).  On linux, meshlab is available through apt.  **Download meshlab, and have a look at the resulting mesh**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!meshlab /home/brinkerhoff/Sandbox/cv_sfm/boot/odm_meshing/odm_mesh.ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a mesh in place, we have a dense representation of the geometry of the scene, but we still don't have the color from the original image.  odm_texturing drapes the original imagery over the mesh.  We can also view this in meshlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!meshlab /home/brinkerhoff/Sandbox/cv_sfm/boot/odm_texturing/odm_textured_model.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the data pipeline above to one of the sequences of images that we generated in class, either from the camera, or from the drone**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
